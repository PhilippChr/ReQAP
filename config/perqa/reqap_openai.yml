### Benchmark
benchmark: 
    benchmark: "perqa"
    benchmark_dir: "./data/benchmarks/perqa"
    result_dir: "./data/results/perqa/reqap_openai"

### ICL
openai: 
    ### OpenAI LLM
    openai_model: "gpt-4o"
    openai_organization: "<YOUR_ORG>"
    openai_project: "<YOUR_PROJECT>"
    openai_key: "<YOUR_KEY>"
    use_cache: False

### QU
qu:
    qu_execution_cache_size: 100  #EFFICIENCY / MEMORY trade-off
    qu_mode_options: ["openai", "instruct_model", "seq2seq"] 
    qu_mode: "openai"
    # qu - supervisor
    qu_supervisor_instr: "./prompts/instruction-qu_supervisor.txt"
    qu_supervisor_icl: "./prompts/icl-qu_supervisor.json"
    qu_icl_selection_strategy: "dynamic"
    qu_icl_num_examples: 8
    qu_inference_batch_size: 512
    qu_result_paths:
        train: "./data/results/perqa/reqap_openai/qu_train.jsonl"
        dev: "./data/results/perqa/reqap_openai/qu_dev.jsonl"
        test: "./data/results/perqa/reqap_openai/qu_test.jsonl"
    # results for training QU SFT model
    qu_training:
        qu_correct_only: False
        qu_result_data:
            train: "./data/training_data/perqa/qu/train_data_results_openai.jsonl"
            dev: "./data/training_data/perqa/qu/dev_data_results_openai.jsonl"
    # qu - model - generation
    sampling_params:
        n: 3
        temperature: 0.1

### RETRIEVE
# splade
splade:
    splade_indices_dir: "./data/splade_indices/perqa"
    splade_model_type_or_path: "./data/models/splade-cocondenser-ensembledistil"
    splade_tokenizer_type: "./data/models/splade-cocondenser-ensembledistil"
    splade_max_length: 512
    splade_index_batch_size: 8
    splade_threshold: 0.1  # threshold for events derived by SPLADE in step 1 of RETRIEVE
    splade_verbalize_events: True  # enables linearization of events for retrieval (slight improvements)
crossencoder:
    # crossencoder - models
    model_patterns: 
        crossencoder_tokenizer_path: "./data/tokenizers/crossencoder-tokenizer-ms-marco-MiniLM-L-12-v2"
        crossencoder_trained_model_path: "./data/models/perqa/crossencoder-trained_model_patterns-ms-marco-MiniLM-L-12-v2"
        max_length: 50
        inference_batch_size: 128
    model_events: 
        crossencoder_tokenizer_path: "./data/tokenizers/crossencoder-tokenizer-ms-marco-MiniLM-L-12-v2"
        crossencoder_trained_model_path: "./data/models/perqa/crossencoder-trained_model_events-ms-marco-MiniLM-L-12-v2"
        max_length: 512
        inference_batch_size: 128  
    # crossencoder - training
    crossencoder_retrieve_calls_train_set: "./data/training_data/perqa/crossencoder/train_retrieve_calls.jsonl"
    crossencoder_retrieve_calls_dev_set: "./data/training_data/perqa/crossencoder/dev_retrieve_calls.jsonl"
    crossencoder_train_equivalent_queries: "./data/training_data/perqa/crossencoder/train_equivalent_queries.json"
    crossencoder_dev_equivalent_queries: "./data/training_data/perqa/crossencoder/dev_equivalent_queries.json"
    crossencoder_train_data: "./data/training_data/perqa/crossencoder/train_data.jsonl"
    crossencoder_dev_data: "./data/training_data/perqa/crossencoder/dev_data.jsonl"
    crossencoder_train_num_positives: 5
    crossencoder_train_num_negatives: 5
    crossencoder_train_num_random_negatives: 5
    crossencoder_train_num_positive_patterns: 5
    crossencoder_train_num_negative_patterns: 5
    crossencoder_train_min_recall: 0.5
    crossencoder_inference_batch_size: 128
    # retrieval pattern
    unified_negative_patterns: False  # whether candidate positive patterns can be used for pruning all (often noisy)
    retrieval_pattern:
        apply: True  # whether to utilize patterns to score events as sets
        min_events_matched_inference: 100  # absolute frequency threshold for patterns during inference
        min_events_matched_train: 100 # absolute frequency threshold for patterns during training RETRIVE

### EXTRACT
extract:
    # extract - model
    tokenizer_path: "./data/tokenizers/extract-tokenizer-bart-base"
    trained_model_path: "./data/models/perqa/extract-trained_model-bart-base"
    max_input_length: 512
    max_output_length: 512
    inference_batch_size: 128
    extract_sample_size_simple_lookup: 50  # number of EXTRACT samples to identify a simple mapping (e.g., date -> start_date)
    simple_lookup_threshold: 0.7 # threshold for exact matches for deriving simple mapping
    extract_add_persona: True  # adds salient information for persona (e.g., list of friends)
    merge_overlapping_events: True  # enables event de-duplication
    # extract - model - training
    extract_training:
        extract_add_persona: True
        num_random_negatives: 1
        num_negatives_same_source: 3
        extract_max_instances:
            train: 100000
            dev: 10000
        extract_attributes_train_path: "./data/training_data/perqa/extract/attributes_train.json"  # shared across datasets
        extract_attributes_dev_path: "./data/training_data/perqa/extract/attributes_dev.json"  # shared across datasets
        extract_attribute_calls:
            train: "./data/training_data/perqa/extract/attribute_calls_train.json"
            dev: "./data/training_data/perqa/extract/attribute_calls_dev.json"
        extract_attribute_mapping_prompt: "./prompts/instruction-attribute_mapping.txt"
        extract_attribute_mapping:
            train: "./data/training_data/perqa/extract/attribute_mapping_train.json"  # shared across datasets
            dev: "./data/training_data/perqa/extract/attribute_mapping_dev.json"  # shared across datasets
        extract_data:
            train:
                very_simple: "./data/training_data/perqa/extract/train_data_very_simple.jsonl"
                mixed: "./data/training_data/perqa/extract/train_data_mixed.jsonl"
                simple: "./data/training_data/perqa/extract/train_data_simple.jsonl"
                negative: "./data/training_data/perqa/extract/train_data_negative.jsonl"
                aliases: "./data/training_data/perqa/extract/train_data_aliases.jsonl"
            dev:
                very_simple: "./data/training_data/perqa/extract/dev_data_very_simple.jsonl"
                mixed: "./data/training_data/perqa/extract/dev_data_mixed.jsonl"
                simple: "./data/training_data/perqa/extract/dev_data_simple.jsonl"
                negative: "./data/training_data/perqa/extract/dev_data_negative.jsonl"
                aliases: "./data/training_data/perqa/extract/dev_data_aliases.jsonl"
    # extract - model - generation
    generation_params:
        num_beams: 1
        no_repeat_ngram_size: 3
        forced_bos_token_id: 0
